{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intraclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19264 images belonging to 16 classes.\n",
      "Found 8248 images belonging to 16 classes.\n",
      "Epoch 1/15\n",
      "62/62 [==============================] - 139s 2s/step - loss: 2.5334 - acc: 0.1411 - val_loss: 2.3835 - val_acc: 0.1862\n",
      "Epoch 2/15\n",
      "62/62 [==============================] - 132s 2s/step - loss: 2.3405 - acc: 0.2026 - val_loss: 2.2525 - val_acc: 0.2675\n",
      "Epoch 3/15\n",
      "62/62 [==============================] - 132s 2s/step - loss: 2.3076 - acc: 0.2359 - val_loss: 2.1980 - val_acc: 0.2350\n",
      "Epoch 4/15\n",
      "62/62 [==============================] - 133s 2s/step - loss: 2.1910 - acc: 0.2626 - val_loss: 2.1318 - val_acc: 0.2750\n",
      "Epoch 5/15\n",
      "62/62 [==============================] - 135s 2s/step - loss: 2.1475 - acc: 0.2898 - val_loss: 2.0856 - val_acc: 0.3375\n",
      "Epoch 6/15\n",
      "62/62 [==============================] - 134s 2s/step - loss: 2.1354 - acc: 0.2989 - val_loss: 2.0153 - val_acc: 0.3088\n",
      "Epoch 7/15\n",
      "62/62 [==============================] - 135s 2s/step - loss: 2.0331 - acc: 0.3311 - val_loss: 2.0750 - val_acc: 0.3538\n",
      "Epoch 8/15\n",
      "62/62 [==============================] - 198s 3s/step - loss: 2.0916 - acc: 0.3075 - val_loss: 1.9690 - val_acc: 0.3287\n",
      "Epoch 9/15\n",
      "62/62 [==============================] - 204s 3s/step - loss: 2.0057 - acc: 0.3589 - val_loss: 1.9034 - val_acc: 0.3775\n",
      "Epoch 10/15\n",
      "62/62 [==============================] - 208s 3s/step - loss: 1.9485 - acc: 0.3528 - val_loss: 2.0333 - val_acc: 0.3212\n",
      "Epoch 11/15\n",
      "62/62 [==============================] - 211s 3s/step - loss: 1.8988 - acc: 0.3841 - val_loss: 1.8377 - val_acc: 0.3977\n",
      "Epoch 12/15\n",
      "62/62 [==============================] - 205s 3s/step - loss: 1.8740 - acc: 0.3831 - val_loss: 1.8882 - val_acc: 0.3725\n",
      "Epoch 13/15\n",
      "62/62 [==============================] - 985s 16s/step - loss: 1.8931 - acc: 0.3921 - val_loss: 1.8339 - val_acc: 0.4075\n",
      "Epoch 14/15\n",
      "62/62 [==============================] - 165s 3s/step - loss: 1.8964 - acc: 0.3679 - val_loss: 1.8168 - val_acc: 0.3850\n",
      "Epoch 15/15\n",
      "62/62 [==============================] - 168s 3s/step - loss: 1.8746 - acc: 0.3987 - val_loss: 1.9072 - val_acc: 0.3713\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'E:/Final project/train/Phytoplankton'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split = 0.3)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    classes = ['artifacts', 'Chaetognaths', 'Copepods', 'Ctenophores', 'detritus', 'diatom', 'Echinoderms', 'Fish','Gastropods', 'hydromedusae', 'Other Invert Larvae', 'Pelagic Tunicates', 'Protists', 'Shrimp', 'Siphonophores', 'trichodesmium'],\n",
    "    subset='training')\n",
    "\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    classes = ['artifacts', 'Chaetognaths', 'Copepods', 'Ctenophores', 'detritus', 'diatom', 'Echinoderms', 'Fish','Gastropods', 'hydromedusae', 'Other Invert Larvae', 'Pelagic Tunicates', 'Protists', 'Shrimp', 'Siphonophores', 'trichodesmium'],\n",
    "    subset='validation')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), input_shape=input_shape))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "#model.add(BatchNormalization())\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(32))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')\n",
    "model.save('plankton2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20590 images belonging to 2 classes.\n",
      "Found 8823 images belonging to 2 classes.\n",
      "Epoch 1/15\n",
      "62/62 [==============================] - 122s 2s/step - loss: 0.7164 - acc: 0.6472 - val_loss: 0.5935 - val_acc: 0.6963\n",
      "Epoch 2/15\n",
      "62/62 [==============================] - 92s 1s/step - loss: 0.6053 - acc: 0.6880 - val_loss: 0.5782 - val_acc: 0.6913\n",
      "Epoch 3/15\n",
      "62/62 [==============================] - 70s 1s/step - loss: 0.6161 - acc: 0.6798 - val_loss: 0.5530 - val_acc: 0.7262\n",
      "Epoch 4/15\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.5850 - acc: 0.7167 - val_loss: 0.5539 - val_acc: 0.7350\n",
      "Epoch 5/15\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.5824 - acc: 0.7082 - val_loss: 0.5593 - val_acc: 0.7200\n",
      "Epoch 6/15\n",
      "62/62 [==============================] - 72s 1s/step - loss: 0.5675 - acc: 0.7339 - val_loss: 0.5391 - val_acc: 0.7350\n",
      "Epoch 7/15\n",
      "62/62 [==============================] - 70s 1s/step - loss: 0.5611 - acc: 0.7157 - val_loss: 0.5760 - val_acc: 0.6925\n",
      "Epoch 8/15\n",
      "62/62 [==============================] - 70s 1s/step - loss: 0.5382 - acc: 0.7414 - val_loss: 0.5258 - val_acc: 0.7675\n",
      "Epoch 9/15\n",
      "62/62 [==============================] - 69s 1s/step - loss: 0.5666 - acc: 0.7152 - val_loss: 0.5400 - val_acc: 0.7500\n",
      "Epoch 10/15\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.5319 - acc: 0.7571 - val_loss: 0.5076 - val_acc: 0.7550\n",
      "Epoch 11/15\n",
      "62/62 [==============================] - 91s 1s/step - loss: 0.5389 - acc: 0.7429 - val_loss: 0.5256 - val_acc: 0.7612\n",
      "Epoch 12/15\n",
      "62/62 [==============================] - 82s 1s/step - loss: 0.5414 - acc: 0.7591 - val_loss: 0.5292 - val_acc: 0.7446\n",
      "Epoch 13/15\n",
      "62/62 [==============================] - 75s 1s/step - loss: 0.5268 - acc: 0.7473 - val_loss: 0.6123 - val_acc: 0.7225\n",
      "Epoch 14/15\n",
      "62/62 [==============================] - 74s 1s/step - loss: 0.5293 - acc: 0.7515 - val_loss: 0.5225 - val_acc: 0.7725\n",
      "Epoch 15/15\n",
      "62/62 [==============================] - 72s 1s/step - loss: 0.5236 - acc: 0.7545 - val_loss: 0.5228 - val_acc: 0.7925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'E:/Trial'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split = 0.3)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    classes = ['phytoplankton', 'zooplankton'],\n",
    "    subset='training')\n",
    "\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    classes = ['phytoplankton', 'zooplankton'],\n",
    "    subset='validation')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "#model.add(BatchNormalization())\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(10))\n",
    "LeakyReLU(alpha=0.1)\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')\n",
    "model.save('plankton5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
